---
title: "PP249, Problem Set 2"
author: "Peter Amerkhanian"
date: "9/14/2022"
output:
  html_document: default
  github_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(knitr)
library(haven)
```

# Question 1 

You are given cleaned data from the Tennessee Star Experiment, which aims to address the impact of class size on student outcomes. This question walks through the following tasks, either in R or Stata (please be sure to share your code!):  
1)	Load in the dataset, which is stored on bCourses as “data_tennessee_star_q1.csv”  
2)	Conduct summary statistics as requested in Question 1.1  
3)	Conduct a regression with robust standard errors, where “math_score” is the outcome and our only predictor variable is “treat”  
4)	Answer the questions below! 

```{r, cache=TRUE}
data_tennessee_star_q1 <- read_csv("data_tennessee_star_q1.csv",
                                   show_col_types = FALSE)
```

## Question 1.1
Before we get into the regression, we want to run some summary statistics. Fill in the table below for the outcome variable, math_score.

```{r}
q1_summary_table <- data_tennessee_star_q1 %>%
  group_by(treat) %>%
  summarise(
    n = n(),
    Minimum = min(math_score),
    Mean = mean(math_score),
    `Standard Deviation` = sd(math_score),
    Maximum = max(math_score)
  ) %>%
  rename(`Treatment Group` = treat)
kable(q1_summary_table)
```
## Question 1.2
Conduct the regression with robust standard errors (using math_score as the outcome and treat as the predictor).

**Answer:**  
*Note: R's `lm()` command will, by default, run a regression with robust standard errors*
```{r}
model1 <- lm(math_score ~ treat, data=data_tennessee_star_q1)
model1_summary <- summary(model1)
model1_summary
```
- What is control group mean?
```{r}
cat(
  model1_summary$coef['(Intercept)', 'Estimate']
)
```
- What is the treatment group mean?
```{r}
cat(
  model1_summary$coef['(Intercept)', 'Estimate'] +
    model1_summary$coef['treat', 'Estimate']
)
```
- What is the estimated treatment effect?
```{r}
cat(
  model1_summary$coef['treat', 'Estimate']
)
```
- What is the estimated standard error on the treatment effect?
```{r}
cat(
  model1_summary$coef["treat","Std. Error"]
)
```
- What is the “t statistic”?
```{r}
cat(
  model1_summary$coef["treat","t value"]
)
```
- Is the impact of treat on math_scores statistically significant?
Yes, it's very significant, with a $p-value$ as follows:
```{r}
cat(
  model1_summary$coef['treat', 'Pr(>|t|)']
  )
```
## Question 1.3
Conduct a two-sample t-test for the difference in means between the treatment group and the control group.  What is the t statistic? Does this differ from your answer to Question 1.2? Did you conduct this testing assuming equal or unequal variances?

**Answer:**  
the R command `t.test()` will default to conducting the two-sample t-test assuming unequal variances. The results, below, are the same as the results I obtained in my regression in Question 1.2.
```{r}
t.test(math_score ~ treat, data=data_tennessee_star_q1)
```
Alternatively, I can conduct the test assuming equal variances by adding `var.equal = TRUE`, and I will get different levels of significance, in this case a more pronounced $t-statistic$ and lower $p-value$, but the point estimates themselves are still the same as the regression. 
```{r}
t.test(math_score ~ treat, data=data_tennessee_star_q1, var.equal = TRUE)
```
## Question 1.4
Consider the case where we double the sample size. Holding all else equal, we would see the following:  

- Standard Error: Increase, Decrease, or Stay the Same?  
Given that the sample size, $n$ is in the denominator of the formula for standard error, **Standard Error decreases as sample size increases.**
```{=latex}
\begin{equation*}
SE=\frac{\sigma}{\sqrt{n}} \\
\frac{\sigma}{\sqrt{n}} > \frac{\sigma}{\sqrt{2n}}
\end{equation*}

```


- t-statistic: Increase, Decrease, or Stay the Same?  
Given the formula for the t statistic below, as sample size increases, $SE$, the denomiator, decreases, and so **the t statistic will increase.**
```{=latex}
\begin{equation*}
t_{\hat{\beta}}=\frac{\hat{\beta} - \beta_0} {SE(\hat{\beta})}
\end{equation*}

```


- p-value: Increase, Decrease, or Stay the Same?  
The p-value, or, probability of a false positive error, will decrease as sample size increases.

# Question 2
You are given cleaned data from the Gerber and Green experiment on the effects of canvassing, direct mail, and phone calls on voter turnout. This question walks through the following tasks, either in R or Stata (please be sure to share your code!):

1)	Load in the dataset, which is stored on bCourses as “data_gerber_q2.csv”  
2)	Conduct a regression with robust standard errors, where “vote” is the outcome and our only predictor variable is “treat”  
3)	Answer the questions below! 
```{r}
data_gerber_q2 <- read_csv("data_gerber_q2.csv",
                          show_col_types = FALSE)
```

## Question 2.1
Conduct the regression with robust standard errors (using vote as the outcome and treat as the predictor). Please answer the following:
```{r}
model2 <- lm(vote ~ treat, data=data_gerber_q2)
model2_summary <- summary(model2)
model2_summary
```
- What is control group mean? 
```{r}
cat(
  model2_summary$coef['(Intercept)', 'Estimate']
  )
```
- What is the treatment group mean?
```{r}
cat(
  model2_summary$coef['(Intercept)', 'Estimate'] +
      model2_summary$coef['treat', 'Estimate']
  )
```
- What is the estimated treatment effect?
```{r}
cat(
  model2_summary$coef['treat', 'Estimate']
  )
```
- What is the estimated standard error on the treatment effect? 
```{r}
cat(
  model2_summary$coef['treat', 'Std. Error']
  )
```
- What is the “t statistic”? 
```{r}
cat(
  model2_summary$coef['treat', 't value']
  )
```
- Is the impact of treat on vote statistically significant? 
**Answer:**  
Yes, I would go as far to say it's extremely significant, with a t statistic that large and with a $p-value$ as follows:
```{r}
cat(
  model2_summary$coef['treat', 'Pr(>|t|)']
  )
```

# Question 3: replicating measures of Black-white inequality as in Bayer and Charles (2018)
The goal of this exercise is to replicate measures of racial gap in log median income (as defined in Bayer and Charles (2018)) for three samples: (i) for working men only, (ii) for all men, and (iii) for all men or women

Please read below:  
1)	Instructions for requesting US Census data from IPUMS.org  
2)	Instructions for running analysis and plotting figures  
3)	Please run “measuring_bw_inequality_cleaning.do” to help you clean the dataset before you start running quantile regressions to measure the racial income level gaps at the median.

```{r}
full_file <- read_dta("usa_00003/usa_00003.dta")
```


This do-file creates a dataset on black-white earnings gaps over 1950-2019 at the median (and 90th percentile) controlling for age groups.

STEPS:
	*0. Set directories.
	*1. Import census and ACS data, adjust for inflation.
	*2. Generate and prepare variables for analysis.
	*3. Perform quantile regression.
	*4. Save regression results as dataset.
	*5. Plot graphs.
	
*first created: 9/8/2022
*last updated:  9/8/2022

*code for limiting years if pulled an extract with too many years
	use "data/raw/usa_00053.dta", clear
	*tab year
	keep inlist(year,1950,1960,1970,1980,1990,2000,2007,2010,2014,2019)
	saveold "data/raw/usa_00053.dta", replace
```{r}
years_vec <- c(1950,
               1960,
               1970,
               1980,
               1990,
               2000,
               2007,
               2010,
               2014,
               2019)
df_years_filtered <- full_file %>% filter(year %in% years_vec)
```

*split dataset into 10 percent samples of each year to make processing manageable
	foreach yr in 1950 1960 1970 1980 1990 2000 2007 2010 2014 2019 {
		use "data/raw/usa_00053.dta", clear
		keep if year==`yr'
		sample 10
		tempfile cenacs`yr'
		save "`cenacs`yr''"
	}
```{r}
df_samples <- df_years_filtered %>% group_by(year) %>% slice_sample(prop=.1)
```

*append 10-percent samples of each year into one master dataset
	use `cenacs1950', clear

	foreach yr in 1950 1960 1970 1980 1990 2000 2007 2010 2014 2019 {
		append using "`cenacs`yr''"
		}
	save "data/output/census_acs_1950_2019.dta", replace
	
*2. Generate and prepare variables for analysis.
*------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------%
 *use "data/output/census_acs_1950_2019.dta", clear
	*Sampling criteria - limited to ages 25-54
		drop if age < 25 | age > 54
```{r}
df_age_filter <- df_samples %>% filter(age >= 25 & age <= 54)
```
	*Adjust income variables given changing definitions over time 
	*Replace observations with inconsistent wage
		drop if incwage >= 999998
```{r}
df_inc_filter <- df_age_filter %>% filter(incwage < 999998)
```
		*replace incbusfm = . if incbusfm == 99999
		replace incbusfm = 0 if incbusfm < 0
		*replace incbus = . if incbus == 999999
		replace incbus = 0 if incbus < 0
		*replace incfarm = . if incfarm == 999999
		replace incfarm = 0 if incfarm < 0
		*replace incbus00 = . if incbus00 == 999999
		replace incbus00 = 0 if incbus00 < 0
```{r}
df_inc_replac <- df_inc_filter %>%
  mutate(incbusfm = replace(incbusfm,
                            incbusfm == 99999,
                            NA)) %>%
  mutate(incbusfm = replace(incbusfm,
                            incbusfm <0,
                            0)) %>%
  mutate(incbusfm = replace(incbus,
                            incbus == 99999,
                            NA)) %>%
  mutate(incbusfm = replace(incbus,
                            incbus < 0,
                            0)) %>%
  mutate(incbusfm = replace(incfarm,
                            incfarm == 99999,
                            NA)) %>%
  mutate(incbusfm = replace(incfarm,
                            incfarm <0,
                            0)) %>%
  mutate(incbusfm = replace(incbus00,
                            incbus00 == 99999,
                            NA)) %>%
  mutate(incbusfm = replace(incbus00,
                            incbus00 <0,
                            0))
  
```
	*Adjust income variables given changing definitions over time 
	replace incwage = 1.2 * incwage if ind1950 == 105
	replace incbusfm = 1.4 * incbusfm if ind1950 == 105 & year <= 1960
	replace incbusfm = incbus + 1.4 * incfarm if year >= 1970 & year <= 1990
	replace incbusfm = incbus00 if year >= 2000
	replace incbusfm = 1.4 * incbus00 if year >= 2000 & ind1950 == 105

	*Sum up income
	gen inc = incwage + incbusfm 
	replace inc = incwage if inc == .

	*merge in inflation numbers
	merge m:1 year using "data/raw/cpi_acs.dta"
	keep if inlist(year,1950,1960,1970,1980,1990,2000,2007,2010,2014,2019)
	drop _m

	* adjust to real 2019 dollars
	gen real_earnings = inc * (376.5/CPI_1977)

	*create samples dummies for 3 different figures
		gen sample1=0
		gen sample2=0
		gen sample3=0

		replace sample1 = 1 if sex==1 & real_earnings>1 /*for working men only*/
		replace sample2 = 1 if sex==1 /*all men*/
		replace sample3 = 1 /*all men and women*/

	* log real earnings
	gen logrealearn = log(real_earnings + 1)

	* genrate age controls
		*gen ageg1 = age > 24 & age < 30* Baseline age group
		gen ageg2 = age > 29 & age < 35
		gen ageg3 = age > 34 & age < 40
		gen ageg4 = age > 39 & age < 45
		gen ageg5 = age > 44 & age < 50
		gen ageg6 = age > 49 & age < 55
		drop age

	* racial dummy variables
	gen black = race == 2 & hispan == 0
	gen white = race == 1 & hispan == 0
	gen other = black == 0 & white == 0

	* racial category variable
	gen 	race_string="Other"
	replace race_string="Black" if race==2
	replace race_string="White" if race==1

*------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------%	
*3. Perform quantile regressions to compute racial earnings level gaps at different percentiles and plot graphs
*------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------%

